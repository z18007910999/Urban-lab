# app.py â€” Urban Lab Â· News Categorizer (manual review; only write to news_reviews)
# deps: streamlit supabase==2.* python-dotenv pandas

import pandas as pd
import streamlit as st
from datetime import date, timedelta, datetime, timezone
from supabase_io import fetch_articles, supabase  # å¤ç”¨ä½ çš„å°è£…ä¸å®¢æˆ·ç«¯
from io import BytesIO
import os, requests, re  # âœ… changed: add re
from docx import Document
from docx.shared import Pt, Inches
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from html import escape
from PIL import Image
import streamlit.components.v1 as components


# ===== Weekly DOCX helpers =====

OUTPUT_DIR = r"D:\Python Project\weekly outcome"  # ç›®æ ‡ä¿å­˜ç›®å½•ï¼ˆå¯åœ¨UIé‡Œæ”¹ï¼‰

def start_of_week(d: date) -> date:
    return d - timedelta(days=d.weekday())  # å‘¨ä¸€

def end_of_week(start: date) -> date:
    return start + timedelta(days=6)

def _add_label_value(doc: Document, label: str, value: str, bold_label=True):
    p = doc.add_paragraph()
    r1 = p.add_run(f"{label} ")
    r1.bold = bold_label
    r1.font.size = Pt(11)
    r2 = p.add_run(value or "")
    r2.font.size = Pt(11)
    return p

def _add_hyperlink(paragraph, url, text):
    part = paragraph.part
    r_id = part.relate_to(url,
                          reltype="http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink",
                          is_external=True)
    hyperlink = OxmlElement('w:hyperlink')
    hyperlink.set(qn('r:id'), r_id)
    new_run = OxmlElement('w:r')
    rPr = OxmlElement('w:rPr')
    u = OxmlElement('w:u'); u.set(qn('w:val'), 'single'); rPr.append(u)
    color = OxmlElement('w:color'); color.set(qn('w:val'), '0563C1'); rPr.append(color)
    new_run.append(rPr)
    t = OxmlElement('w:t'); t.text = text
    new_run.append(t)
    hyperlink.append(new_run)
    paragraph._p.append(hyperlink)

# =========================
# Image fetching utilities
# =========================

# ç»Ÿä¸€çš„è¿œç¨‹å›¾ç‰‡ä¸‹è½½ï¼ˆrequests + PIL éªŒè¯ï¼‰
def fetch_remote_img(url: str) -> bytes | None:
    """
    ä¸‹è½½è¿œç¨‹å›¾ç‰‡å¹¶è¿”å›äºŒè¿›åˆ¶ï¼›è‹¥ä¸æ˜¯å›¾ç‰‡æˆ–å¤±è´¥åˆ™è¿”å› Noneã€‚
    """
    if not url:
        return None
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        # ç”¨ PIL éªŒè¯æ˜¯å¦ä¸ºå›¾ç‰‡
        img = Image.open(BytesIO(resp.content))
        img.verify()
        return resp.content
    except Exception:
        return None

# âœ… changed: ä½ æä¾›çš„ curl å¤´å’Œ cookieï¼ˆå¯æŒ‰éœ€æ›´æ–°ï¼‰
NYT_HEADERS = {
    
def fetch_og_image_url_with_curl(page_url: str) -> str | None:
    """
    ç”¨ curl ç­‰ä»·çš„ headers + cookie æŠ“é¡µé¢ï¼Œè§£æ og:imageã€‚
    """
    if not page_url:
        return None
    try:
        headers = NYT_HEADERS.copy()
        headers["Cookie"] = NYT_COOKIE
        resp = requests.get(page_url, headers=headers, timeout=12, allow_redirects=True)
        resp.raise_for_status()
        html = resp.text
        m = re.search(
            r'<meta[^>]+property=["\']og:image["\'][^>]*content=["\']([^"\']+)["\']',
            html, flags=re.I
        )
        if not m:
            return None
        og = m.group(1).strip()
        if og.startswith("//"):
            og = "https:" + og
        return og
    except Exception:
        return None

@st.cache_data(show_spinner=False, ttl=3600)
def _fetch_reviews_week(monday: date):
    """ä»å®¡æ ¸è¡¨è¯»å–æœ¬å‘¨ [Mon..Sun] çš„è®°å½•ï¼›è¡¨åä¼˜å…ˆ news_reviewsï¼Œå›é€€ '\"News_reviews\"'ã€‚"""
    start_s, end_s = monday.isoformat(), end_of_week(monday).isoformat()
    table_candidates = ["news_reviews", '"News_reviews"']
    last_err = None
    for tbl in table_candidates:
        try:
            res = (
                supabase.table(tbl)
                .select("*")
                .gte("publish_date", start_s)
                .lte("publish_date", end_s)
                .order("publish_date", desc=False)
                .execute()
            )
            data = res.data or []
            if data is not None:
                return data
        except Exception as e:
            last_err = e
            continue
    raise RuntimeError(f"Read reviews failed: {last_err}")

def build_weekly_docx(rows: list[dict], monday: date, author: str) -> BytesIO:
    """æŒ‰æ¨¡æ¿ç”Ÿæˆ DOCX å¹¶è¿”å›å­—èŠ‚ç¼“å†²ï¼ˆä¾›ä¸‹è½½/å¦å­˜ï¼‰ã€‚"""
    week_text = monday.strftime("%B %d, %Y")  # e.g., October 27, 2025
    doc = Document()

    for i, r in enumerate(rows):
        if i > 0:
            doc.add_page_break()

        # Week of
        p_week = doc.add_paragraph()
        run = p_week.add_run(f"Week of {week_text}")
        run.bold = True; run.font.size = Pt(12)

        # Title
        _add_label_value(doc, "Title:", r.get("title",""))

        # Source / Date Published / Link / Author
        _add_label_value(doc, "Source:", r.get("publisher",""))
        pubdate = r.get("publish_date")
        pubdate_str = ""
        try:
            if pubdate:
                pubdate_str = pd.to_datetime(pubdate).strftime("%m.%d.%Y")
        except Exception:
            pass
        _add_label_value(doc, "Date Published:", pubdate_str)

        p_link = doc.add_paragraph()
        r_label = p_link.add_run("Link: "); r_label.bold = True; r_label.font.size = Pt(11)
        link = r.get("link") or r.get("url") or ""
        if link:
            _add_hyperlink(p_link, link, link)

        _add_label_value(doc, "Urban Lab Author:", author)

        # âœ… changed: Article Photographï¼Œä¼˜å…ˆ image_urlï¼Œå†å°è¯•ç”¨ curl å¤´ä»æ–‡ç« é¡µæŠ“ og:image
        image_url = (r.get("image_url") or "").strip()
        img_bytes = None
        if image_url:
            img_bytes = fetch_remote_img(image_url)
        if (not img_bytes) and link:
            og_url = fetch_og_image_url_with_curl(link)
            if og_url:
                img_bytes = fetch_remote_img(og_url)

        if img_bytes:
            doc.add_paragraph("Article Photograph:")
            try:
                doc.add_picture(BytesIO(img_bytes), width=Inches(6.5))
            except Exception:
                doc.add_paragraph("")
        else:
            _add_label_value(doc, "Article Photograph:", "")

        # Summary
        p_sum = doc.add_paragraph()
        r1 = p_sum.add_run("Article Summary: "); r1.bold = True; r1.font.size = Pt(11)
        p_sum.add_run(r.get("summary","")).font.size = Pt(11)

        # Initiativesï¼ˆçº¢è‰²/åŠ ç²—ï¼‰
        p_init = doc.add_paragraph()
        r2 = p_init.add_run("Initiative: "); r2.bold = True; r2.font.size = Pt(11)
        r3 = p_init.add_run(r.get("categories","")); r3.bold = True; r3.font.size = Pt(11)

    bio = BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio

def build_weekly_block_text(row: dict, categories: str, author: str) -> str:
    """
    ç”Ÿæˆä¸€ä¸ªçº¯æ–‡æœ¬ç‰ˆçš„ Weekly Report å—ï¼Œç”¨äºåœ¨é¡µé¢ä¸Šç›´æ¥å¤åˆ¶åˆ° Google Docsã€‚
    """
    pubdate = row.get("publish_date")
    # è®¡ç®— Week of
    monday_text = ""
    try:
        if isinstance(pubdate, date):
            d = pubdate
        else:
            d = pd.to_datetime(pubdate).date()
        monday_text = start_of_week(d).strftime("%B %d, %Y")
    except Exception:
        pass

    # æ ¼å¼å°½é‡å’Œ DOCX ä¿æŒä¸€è‡´
    lines = []
    if monday_text:
        lines.append(f"Week of {monday_text}")
        lines.append("")

    lines.append(f"Title: {row.get('title', '')}")
    lines.append(f"Source: {row.get('publisher', '')}")

    # Date Published æ ¼å¼  MM.DD.YYYY
    pub_str = ""
    try:
        if isinstance(pubdate, date):
            pub_str = pubdate.strftime("%m.%d.%Y")
        elif pubdate:
            pub_str = pd.to_datetime(pubdate).strftime("%m.%d.%Y")
    except Exception:
        pub_str = str(pubdate or "")
    lines.append(f"Date Published: {pub_str}")

    link = row.get("url") or row.get("link") or ""
    lines.append(f"Link: {link}")

    lines.append(f"Urban Lab Author: {author}")
    lines.append("")
    lines.append("Article Summary:")
    lines.append(row.get("summary", "") or "")
    lines.append("")
    lines.append(f"Initiative: {categories or ''}")

    return "\n".join(lines)


st.set_page_config(page_title="Urban Lab Â· News Categorizer", page_icon="ğŸ“°", layout="wide")
st.markdown("""
<style>
/* ä¿è¯æ‰€æœ‰åˆ—ä¸ Markdown å®¹å™¨ä¸è¶…å‡ºçˆ¶å®¹å™¨ */
.block-container, .stMarkdown, .stColumn {
  max-width: 100% !important;
}

/* è®© Markdown æ–‡æœ¬å¯ä»¥åœ¨ä»»æ„ä½ç½®æ¢è¡Œï¼Œé¿å…æŒ¤å‡ºå³ä¾§åˆ— */
.stMarkdown p, .stMarkdown div, .stMarkdown span {
  overflow-wrap: anywhere !important;
  word-break: break-word !important;
  white-space: normal !important;
}

/* åˆ—å®¹å™¨ä¸å…è®¸æŠŠå­å…ƒç´ â€œæŒ¤å‡ºâ€ */
[data-testid="stVerticalBlock"] {
  overflow: hidden !important;
}

/* DataFrame æœ¬èº«å…è®¸æ¨ªå‘æ»šåŠ¨ï¼Œä½†ä¸çªç ´åˆ—å®½ */
[data-testid="stDataFrame"] {
  max-width: 100% !important;
  overflow-x: auto !important;
}
</style>
""", unsafe_allow_html=True)
st.title("ğŸ“° Urban Lab â€” News Article Categorization")

CATEGORIES = [
    "Housing Affordability",
    "Culture Led Development",
    "Net Zero Cities",
    "Public/Private Development",
]

# ---------------------------
# æ•°æ®åŠ è½½ï¼ˆä» News_storageï¼‰
# ---------------------------
@st.cache_data(show_spinner=False, ttl=300)
def load_articles(limit: int = 1000) -> pd.DataFrame:
    rows = fetch_articles(limit=limit)
    recs = []
    for r in rows:
        recs.append({
            "id":           r.get("id"),
            "title":        r.get("title", ""),
            "publisher":    r.get("Publisher") or r.get("creator",""),
            "publish_date": r.get("pubdate",""),
            "url":          r.get("link",""),
            "summary":      r.get("summary",""),
            "category":     r.get("Category") or "",
            "image_url":    r.get("image_url",""),
        })
    df = pd.DataFrame(recs)
    if not df.empty:
        df["publish_date"] = pd.to_datetime(df["publish_date"], errors="coerce").dt.date
    return df

df_all = load_articles()
if df_all.empty:
    st.info("Supabase è¡¨ `News_storage` æš‚æ— æ•°æ®æˆ–è¯»å–å¤±è´¥ã€‚è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡å’Œ RLSã€‚")
    st.stop()

# ---------------------------
# ä¾§è¾¹æ ç­›é€‰
# ---------------------------
with st.sidebar:
    st.subheader("Filters")

    min_d = df_all["publish_date"].min()
    max_d = df_all["publish_date"].max()
    if pd.isna(min_d) or pd.isna(max_d):
        from_d, to_d = None, None
    else:
        from_d = st.date_input("Start date", value=min_d, min_value=min_d, max_value=max_d)
        to_d   = st.date_input("End date",   value=max_d, min_value=min_d, max_value=max_d)

    all_pubs = sorted([p for p in df_all["publisher"].dropna().unique() if str(p).strip()])
    sel_pubs = st.multiselect("News Publisher", all_pubs, default=all_pubs)

    q = st.text_input("Search title/summary", value="", placeholder="type keywords").strip()
    only_unreviewed = st.toggle("Show only unreviewed (Category is NULL/empty)", value=False)

# åº”ç”¨ç­›é€‰
df = df_all.copy()
if from_d and to_d:
    df = df[(df["publish_date"].notna()) & (df["publish_date"] >= from_d) & (df["publish_date"] <= to_d)]
if sel_pubs:
    df = df[df["publisher"].isin(sel_pubs)]
if q:
    ql = q.lower()
    df = df[df.apply(lambda r: ql in (r["title"] or "").lower()
                              or ql in (r["summary"] or "").lower(), axis=1)]
if only_unreviewed:
    df = df[(df["category"].isna()) | (df["category"] == "")]

# ---------------------------
# ä¸‰åˆ—å¸ƒå±€
# ---------------------------
left, mid, right = st.columns([3.2, 6, 3.2], gap="large")

# å·¦åˆ—ï¼šæ–‡ç« é€‰æ‹©ï¼ˆåªæ˜¾ç¤ºæ ‡é¢˜ï¼‰
with left:
    st.subheader("Select Article")

    # âœ… ä¸å†æ˜¾ç¤ºè¡¨æ ¼ï¼Œåªæ„å»ºä¸‹æ‹‰é€‰é¡¹ï¼ˆæ˜¾ç¤ºæ ‡é¢˜ï¼‰
    options = [(int(r["id"]), r["title"]) for _, r in df[["id", "title"]].iterrows()]
    if not options:
        st.warning("å½“å‰ç­›é€‰ç»“æœä¸ºç©ºï¼Œè¯·è°ƒæ•´ Filtersã€‚")
        st.stop()

    current = st.selectbox(
        "Select an article",
        options,
        format_func=lambda x: x[1],   # åªæ˜¾ç¤ºæ ‡é¢˜
        index=0,
    )
    current_id = current[0]

# å½“å‰æ–‡ç« ï¼ˆä¿æŒåŸå†™æ³•ï¼‰
row = df.set_index("id").loc[current_id].to_dict()

# ---------------------------
# ä¸­åˆ—ï¼šå®¡æ ¸é¢æ¿ï¼ˆAI Pre-selection = Categoryï¼‰
# ---------------------------
with mid:
    st.subheader("News Categorizer")

    if row.get("url"):
        st.markdown(f"### {row['title']}  â†—")
    else:
        st.markdown(f"### {row['title']}")

    # âœ… changed: Article image æ¸²æŸ“é€»è¾‘
    shown_image = False

    # 1) ä¼˜å…ˆä½¿ç”¨å­˜å‚¨å­—æ®µ image_urlï¼ˆç›´æ¥å½“å›¾ç‰‡ URL ä¸‹è½½ï¼‰
    img_url = (row.get("image_url") or "").strip()
    if img_url:
        img_bytes = fetch_remote_img(img_url)
        if img_bytes:
            st.image(img_bytes, caption="Article image", use_container_width=True)
            shown_image = True

    # 2) å›é€€ï¼šä»æ–‡ç« é¡µæŠ“ og:imageï¼ˆä½¿ç”¨ä½ çš„ curl å¤´å’Œ cookieï¼‰
    if not shown_image and row.get("url"):
        og_url = fetch_og_image_url_with_curl(row["url"])
        if og_url:
            img_bytes2 = fetch_remote_img(og_url)
            if img_bytes2:
                st.image(img_bytes2, caption="Article image", use_container_width=True)
                shown_image = True

    if not shown_image:
        st.info("No image available.")

    # --- Summary ---
    raw_summary = row.get("summary", "")
    if raw_summary:
        # å°½é‡å®‰å…¨è½¬æ–‡æœ¬ï¼Œé¿å… HTML, NaN, None ç­‰é—®é¢˜
        try:
            text = str(raw_summary)
        except Exception:
            text = repr(raw_summary)

        safe_summary = escape(text, quote=False)
        st.markdown(
            f'<div class="stMarkdown">{safe_summary}</div>',
            unsafe_allow_html=True
        )
    else:
        st.markdown("<div class='stMarkdown'>(No summary available)</div>", unsafe_allow_html=True)

    # Week of / Publisher / Publish date
    week_of = None
    if isinstance(row.get("publish_date"), date):
        week_of = (row["publish_date"] - timedelta(days=row["publish_date"].weekday())).isoformat()
    meta = (
        (f"**Publisher:** {row.get('publisher','')}  " if row.get("publisher") else "") +
        (f"**Publish date:** {row.get('publish_date','')}  " if row.get("publish_date") else "")
    )
    if meta:
        st.markdown(meta)

    if row.get("url"):
        st.link_button("Open article â†—", row["url"], use_container_width=True)

    # --- AI é¢„é€‰ = Categoryï¼Œè§„èŒƒåŒ–å¹¶å‹¾é€‰ ---
    st.markdown("#### Recommended Categories (AI Pre-selection)")

    def normalize_cat(s: str) -> str:
        s = (s or "").strip()
        canon = {
            "housing affordability": "Housing Affordability",
            "culture led development": "Culture Led Development",
            "culture-led development": "Culture Led Development",
            "net zero cities": "Net Zero Cities",
            "public/private development": "Public/Private Development",
            "public / private development": "Public/Private Development",
        }
        return canon.get(s.lower(), s)

    preselected = []
    if row.get("category"):
        preselected = [
            normalize_cat(x)
            for x in str(row["category"]).split(";")
            if normalize_cat(x) in CATEGORIES
        ]
    sel = set(preselected)

    cols = st.columns(2)
    with cols[0]:
        c1 = st.checkbox("Housing Affordability", value=("Housing Affordability" in sel))
        c2 = st.checkbox("Net Zero Cities", value=("Net Zero Cities" in sel))
    with cols[1]:
        c3 = st.checkbox("Public/Private Development", value=("Public/Private Development" in sel))
        c4 = st.checkbox("Culture Led Development", value=("Culture Led Development" in sel))

    selected_categories = [
        name for name, flag in [
            ("Housing Affordability", c1),
            ("Net Zero Cities", c2),
            ("Public/Private Development", c3),
            ("Culture Led Development", c4),
        ] if flag
    ]
    categories_str = "; ".join(selected_categories)

    st.divider()

    # --- äººå·¥å®¡æ ¸ï¼Œä»…å†™å…¥ news_reviews ---
    st.markdown("#### Manual Review")
    decision = st.radio("Decision", ["Confirm", "Reject"], horizontal=True, index=0)
    note = st.text_area("Notes (optional)", value="", placeholder="Add reviewer notes...")

    col_save, col_sp = st.columns([1, 3])
    # ä¿å­˜å®¡æ ¸ï¼ˆSave Reviewï¼‰è¿™æ®µï¼šä»…æ›¿æ¢ reviewed_at è¿™ä¸€è¡Œ
    with col_save:
        if st.button("ğŸ’¾ Save Review", use_container_width=True):
            try:
                supabase.table("news_reviews").insert({
                    "title": row.get("title", ""),
                    "publisher": row.get("publisher", ""),
                    "publish_date": str(row.get("publish_date") or ""),
                    "link": row.get("url", ""),
                    "decision": decision.lower(),          # confirm / reject
                    "categories": categories_str,          # '; ' åˆ†éš”
                    "note": note,
                    "summary": row.get("summary", ""),
                    "reviewed_at": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
                }).execute()
                st.success("Review saved to table `news_reviews`.")

                # âœ… å¦‚æœæ˜¯ Confirmï¼ŒæŠŠå½“å‰è¿™æ¡æ–‡ç« çš„ä¿¡æ¯å­˜åˆ° session_stateï¼Œç”¨äºä¸‹é¢æ˜¾ç¤ºæ¨¡æ¿
                if decision.lower() == "confirm":
                    st.session_state["last_confirmed"] = {
                        "row": row,
                        "categories": categories_str,
                    }

            except Exception as e:
                st.error(f"Insert to `news_reviews` failed: {e}")



# ---------------------------
# å³åˆ—ï¼šç»Ÿè®¡ä¸å¤–é“¾
# ---------------------------
with right:
    st.subheader("Weekly Articles")
    st.metric("Count in view", len(df))
    cnt = df.groupby("publisher", dropna=True).size().reset_index(name="count").sort_values("count", ascending=False)
    st.dataframe(cnt.rename(columns={"publisher":"source"}), use_container_width=True, height=360)

with st.expander("ğŸ“ Generate Weekly DOCX Report", expanded=False):
    # å‘¨ä¸€é€‰æ‹©ï¼ˆé»˜è®¤å½“å‰å‘¨å‘¨ä¸€ï¼‰
    today = date.today()
    default_monday = today - timedelta(days=today.weekday())
    monday = st.date_input("Week (pick the Monday)", value=default_monday)

    author = st.text_input("Urban Lab Author", value="Your Name", key="report_author")
    outdir = st.text_input("Save to directory", value=OUTPUT_DIR,
                           help="æœ¬åœ°ä¿å­˜è·¯å¾„ï¼›åŒæ—¶ä¼šæä¾›åœ¨çº¿ä¸‹è½½")

    gen_col1, gen_col2 = st.columns([1,2])
    with gen_col1:
        gen_btn = st.button("Generate DOCX", type="primary", use_container_width=True)

    if gen_btn:
        try:
            rows = _fetch_reviews_week(monday)
            if not rows:
                st.warning("æœ¬å‘¨æš‚æ— å®¡æ ¸è®°å½•ã€‚")
            else:
                docx_bytes = build_weekly_docx(rows, monday, author)
                # 1) åœ¨çº¿ä¸‹è½½
                fname = f"UrbanLab_Weekly_{monday.isoformat()}.docx"
                st.download_button("Download DOCX", data=docx_bytes, file_name=fname, mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")

                # 2) æœ¬æœºä¿å­˜
                try:
                    os.makedirs(outdir, exist_ok=True)
                    save_path = os.path.join(outdir, fname)
                    with open(save_path, "wb") as f:
                        f.write(docx_bytes.getvalue())
                    st.success(f"å·²ä¿å­˜åˆ°ï¼š{save_path}")
                except Exception as e:
                    st.warning(f"ä¿å­˜åˆ°æœ¬åœ°å¤±è´¥ï¼š{e}")
        except Exception as e:
            st.error(f"ç”Ÿæˆå¤±è´¥ï¼š{e}")

# ----------------------------------------
# å•ä¸€æ–‡ç« çš„ Summary æ¨¡æ¿å±•ç¤ºï¼ˆæ ¼å¼å¯¹é½æˆªå›¾ï¼‰
# ----------------------------------------
st.markdown("---")
st.subheader("ğŸ“„ Format to use for Summary (copy & paste)")

data = st.session_state.get("last_confirmed")

if not data:
    st.info("Once you complete the Confirm action with Save above, a copy-ready Weekly Report summary will appear here.")
else:
    r = data["row"]
    cats = data["categories"]
    author_for_block = st.session_state.get("report_author")

    # å¤„ç†æ—¥æœŸ
    pubdate = r.get("publish_date")
    pub_str = ""
    try:
        if isinstance(pubdate, date):
            pub_str = pubdate.strftime("%m.%d.%Y")
        elif pubdate:
            pub_str = pd.to_datetime(pubdate).strftime("%m.%d.%Y")
    except Exception:
        pub_str = str(pubdate or "")

    link = r.get("url") or r.get("link") or ""

    from html import escape as _esc

    # é¡¶éƒ¨æ–‡å­— + å­—æ®µï¼ˆTitle / Source / Date / Link / Author / Article Photographï¼‰
    top_html = f"""

    <p><b>Title:</b> {_esc(r.get('title', '') or '')}</p>
    <p><b>Source:</b> {_esc(r.get('publisher', '') or '')}</p>
    <p><b>Date Published:</b> {_esc(pub_str)}</p>
    <p><b>Link:</b> <a href="{_esc(link)}">{_esc(link)}</a></p>
    <p><b>Urban Lab Author:</b> {_esc(author_for_block)}</p>
    <p><b>Article Photograph:</b></p>
    """
    st.markdown(top_html, unsafe_allow_html=True)

    # å›¾ç‰‡ï¼ˆä¼˜å…ˆ image_urlï¼Œå†å›é€€ og:imageï¼‰
    img_bytes = None
    img_url = (r.get("image_url") or "").strip()
    if img_url:
        img_bytes = fetch_remote_img(img_url)
    if (not img_bytes) and link:
        og_url = fetch_og_image_url_with_curl(link)
        if og_url:
            img_bytes = fetch_remote_img(og_url)

    if img_bytes:
        st.image(img_bytes, width=400)
    else:
        st.write("(No image available)")

    # Summary + Initiativeï¼ˆçº¢è‰²åŠ ç²—ï¼‰
    summary_text = r.get("summary", "") or ""
    bottom_html = f"""
    <p><b>Article Summary:</b> {_esc(summary_text)}</p>
    <p><span style="color: red; font-weight: bold;">
        Initiative: {_esc(cats or '')}
    </span></p>
    """
    st.markdown(bottom_html, unsafe_allow_html=True)

        # ---------- Copy to clipboard button (HTML, ä¿ç•™æ ¼å¼) ----------
    # è¿™æ®µ HTML ä¼šè¢«å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼ŒGoogle Docs ä¼šæŒ‰å¯Œæ–‡æœ¬ç²˜è´´
    clipboard_html = f"""
    <p><b>Title:</b> {_esc(r.get('title', '') or '')}<br>
    <b>Source:</b> {_esc(r.get('publisher', '') or '')}<br>
    <b>Date Published:</b> {_esc(pub_str)}<br>
    <b>Link:</b> <a href="{_esc(link)}">{_esc(link)}</a><br>
    <b>Urban Lab Author:</b> {_esc(author_for_block)}<br>
    <b>Article Photograph:</b> [insert image here]</p>

    <p><b>Article Summary:</b> {_esc(summary_text)}</p>

    <p><b>Initiative:</b> <span style="color: red; font-weight: bold;">
        {_esc(cats or '')}
    </span></p>
    """

    # é¿å…åœ¨ JS æ¨¡æ¿å­—ç¬¦ä¸²é‡ŒæŠŠ ` å’Œ </script> æå
    js_safe_html = (
        clipboard_html
        .replace("\\", "\\\\")
        .replace("`", "\\`")
        .replace("</script>", "<\\/script>")
    )

    components.html(
        f"""
        <button onclick="copySummaryHtml()"
                style="margin-top:8px;padding:6px 12px;font-size:14px;">
            Copy summary
        </button>
        <script>
        async function copySummaryHtml() {{
            const html = `{js_safe_html}`;
            const type = "text/html";
            const blob = new Blob([html], {{ type }});
            const data = [new ClipboardItem({{ [type]: blob }})];
            try {{
                await navigator.clipboard.write(data);
                alert("Summary copied to clipboard with formatting.");
            }} catch (e) {{
                alert("Copy failed: " + e);
            }}
        }}
        </script>
        """,
        height=60,
    )
